{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Press M to activate markdown mode\n",
    "\n",
    "So, what is collection in tensorflowï¼Ÿ In fact, you just take collection as a global container.\n",
    "collection will store all Variables added as a list. You can use get function through name obtain the list.\n",
    "For simple understanding, you can take each collection as a list with a name.\n",
    "In general, we need call add_n to sum losses up when refer to regularizition loss\n",
    "\n",
    "\n",
    "\n",
    "this section, I capsule layer as a function. In the mean time, we use exponential decay learning rate as a fine tuning\n",
    "mechanism. the learning_rate will become less and less with the iteration. \n",
    "However, the change follows a stair down procedure, that is the learning rate keeps still for some iterations, say 100 times.\n",
    "after 100 times iterations. the learning rate goes down for a step\n",
    "you need set several variables in the formula, which is:\n",
    "1. set global_step as 0\n",
    "for each iteration, update the learning_rate as:\n",
    "    decay_learning_rate = base_learning_rate * decay_rate ^ (global_step/decay_rate)\n",
    "if you do not apply decay, base_learning_rate will be your choice. The decay rate is usually set as a < 1 variable.\n",
    "from the formula, we can obtain that, with the bigger global_step, decay_rate ^ (global_step/decay_rate) will decay more.\n",
    "That is, learning rate will be less after sufficient iterations.\n",
    "\n",
    "\n",
    "Another decay mechanism we need apply is average exponential average average for each parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helloworldrewrsdfa\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "avg_class None\n",
      "avg_class None\n",
      "avg_class None\n",
      "1000 0.5321848\n",
      "2000 0.5321303\n",
      "3000 0.5321186\n",
      "4000 0.5321133\n",
      "5000 0.53211033\n",
      "6000 0.5321097\n",
      "7000 0.53211236\n",
      "8000 0.5321084\n",
      "9000 0.53210825\n",
      "10000 0.5321111\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### You must restart the kernel each time when running the program through jupyter\n",
    "#### because collection is a global data struct which is still in memory\n",
    "#### after last running.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import pdb\n",
    "\n",
    "learning_rate = 0.1\n",
    "batch_size = 1000\n",
    "regular_rate = 0.1\n",
    "\n",
    "moving_average_decay = 0.99\n",
    "\n",
    "print(\"helloworldrewrsdfa\")\n",
    "#######for regularizition\n",
    "regularizer = tf.contrib.layers.l2_regularizer\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "######## for exponentail moving average\n",
    "#variables_averages = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)\n",
    "#variables_average_op = variables_averages.apply(tf.trainable_variables())\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "variable_averages = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "def layer(input_size:int, output_size:int, prelayer, avg_class, act_func):\n",
    "    w = tf.Variable(tf.random_normal([input_size, output_size], stddev=0.1))\n",
    "    b = tf.Variable(tf.random_normal([output_size], stddev=0.1))\n",
    "    tf.add_to_collection('losses', regularizer(0.001)(w))\n",
    "    print(\"avg_class\", avg_class)\n",
    "    if avg_class != None:\n",
    "        print(w, b)\n",
    "        #pdb.set_trace()\n",
    "        aw = avg_class.average(w)\n",
    "        ab = avg_class.average(b)\n",
    "        print(prelayer, aw, ab)\n",
    "        aaa = tf.matmul(prelayer, aw) +ab\n",
    "        ret = act_func(aa)\n",
    "        return ret\n",
    "    return act_func(tf.matmul(prelayer, w) + b)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 1), name='x')\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y_')\n",
    "\n",
    "\n",
    "# build net without average class\n",
    "L1 = layer(1, 3, x, None, tf.nn.relu)\n",
    "L2 = layer(3, 2, L1, None, tf.nn.relu)\n",
    "y = layer(2, 1, L2, None, tf.nn.sigmoid)\n",
    "\n",
    "\n",
    "##data\n",
    "size = 1000\n",
    "X = np.random.normal(0, 1, size)\n",
    "Y = [0. if lx<0 else 1. for lx in X]\n",
    "\n",
    "#av_L1 = layer(1, 3, x, variable_averages, tf.nn.relu)\n",
    "#av_L2 = layer(3, 2, av_L1, variable_averages, tf.nn.relu)\n",
    "#av_y = layer(2, 1, av_L2, variable_averages, tf.nn.sigmoid)\n",
    "\n",
    "######for exponential decay\n",
    "###1. learning_rate, 2.global_step, 3.decay_steps, 4.decay_rate, 5.staircase, 6.name\n",
    "learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase=True)\n",
    "\n",
    "closs = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=tf.clip_by_value(y, 1e-8, 1.0)))\n",
    "\n",
    "#mse_loss = tf.reduce_mean(tf.square(y_-y))\n",
    "\n",
    "##############\n",
    "#tf.add_to_collection('losses', mse_loss)\n",
    "tf.add_to_collection('losses', closs)\n",
    "losses= tf.get_collection('losses')\n",
    "loss = tf.add_n(losses, name='loss')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "train_op = tf.group(optimizer, variables_averages_op)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(10000):\n",
    "        start = (i*batch_size)%size\n",
    "        end = min(start+batch_size, size) \n",
    "        xs, ys = X[start:end].reshape(end-start, 1), np.array(Y[start:end]).reshape(end-start, 1)\n",
    "        #print(start, end, len(xs), len(ys))\n",
    "        sess.run(train_op, feed_dict={x:xs, y_:ys})\n",
    "        if i%1000==999:\n",
    "            print(i+1, sess.run(tf.reduce_mean( sess.run(loss, feed_dict={x:xs, y_:ys}) )))\n",
    "    xx = np.linspace(-5,5, 1000).reshape(1000, 1)\n",
    "    #y = sess\n",
    "    py = sess.run(y, feed_dict={x:xx})\n",
    "    plt.plot(xx, py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "v1 = tf.get_variable(name='v1', shape=[1], initializer=tf.constant_initializer(5))  \n",
    "tf.add_to_collection('loss', v1)  \n",
    "v2 = tf.get_variable(name='v2', shape=[1], initializer=tf.constant_initializer(8))  \n",
    "tf.add_to_collection('loss', v2)  \n",
    "  \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())  \n",
    "    print(tf.get_collection('loss'))\n",
    "    print(sess.run(tf.add_n(tf.get_collection('loss'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hello world \n",
    "### you can say something\n",
    "1. sfsf\n",
    "1. fdsf\n",
    "1. fdsfjskl\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
